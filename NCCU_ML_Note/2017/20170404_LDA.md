# Fisher Approach

## Goals

 - Taking linear combinations a'X of the attributes in T so that different classes can be best distinguished.
 - $max \frac{a'Ba}{a'Wa}$ 

## Steps

- 計算每一組k的 $x_k$ 
- 計算不分組的 $\bar{x}$
- 計算 between-group variance matrix 
- 計算 within-group variance matrix
- 排序$W^{-1}B$ eigenvalue，取 r eigenvector, $p$ X  $r$ dimension matrix $W^{-1}B$
    + 所有可得解為 min(K-1, p) = r

## How to classify new object ? 

<center>
    <img src="https://unityculture.github.io/2015_MLCourseNote/NCCU_ML_Note/2017/img/img3.jpg" style="width: 40%; height: 40%" />​
</center>

在Step5得到的 Linear Discriminants（LD, 即 $W^{-1}B$的Eigenvector）可以看作一種投影(線性組合)。如果r = 2的情況，會將新的資料投影到LD Space。比較新資料在LD Space中與k個類別中心的距離，類別預測的判定則基於最短距離。

實際上我們可以投影到的 r 可以選擇在 ≤ min(K-1, p)

# Bayesian Approach

## Goals

- $P_r(Y=k | X=x) = \frac{\pi_k   f_k (x)}{\sum_{l=1}^{K} \pi_l   f_l (x)}$
- To estimate  $\pi_k$ and $f_k (x)$ from  $\frac{\pi_k   f_k (x)}{\sum_{l=1}^{K} \pi_l   f_l (x)}$
- Assume Normal Distribution & Common Variance on $f_x(x)$, if Common Variance is violated, then uses QDA.

## How to classify new object ? 

#### Discriminant Functions :

$\delta_k(x) = x^T\sum^{-1}\mu_k - \frac{1}{2}\mu_k^T\sum^{-1}\mu_{k} + log(\pi_k)$

歸類給 K 當 $\delta_k(x)$   

#### Boundary Decision :

Boundary發生在 $\delta_i(x)$ = $\delta_j(x)$ , for all $i, j \in k$


# Questions

>- 今天花了一個小時了解 Fisher（線代解） & Bayesian Approach（課本解） 的差異。
但還是無法抓到重點。希望LDA或是其他夥伴可以幫忙理解看看XD
http://stats.stackexchange.com/questions/87975/bayesian-and-fishers-approaches-to-linear-discriminant-analysis
整理一些我目前了解的片面結論：
1. Bayesian Assume normal & common variance. 若違反variance的假設 就變成QDA去解
2. Fisher 有 Dimension Reduction 的效果，可以降到 min( 組別數-1, 變數個數）以下的維度
3. Why min(k-1, p)?

>- PCA vs LDA ?   
**相似之處** : 兩者都是用線性組合技巧
PCA : unsupervised, 忽略class labels, goal : 找到最大變異的方向
LDA : supervised, goal : 找到切割越好的軸，投影上去
<img src="http://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png" style="width: 40%; height: 40%"/>​
[PCA vs LDA 參考](http://sebastianraschka.com/Articles/2014_python_lda.html#a-comparison-of-pca-and-lda)

> - Logistic Regression vs LDA

> - Multiple variables, how to calculate between-group variance matrix, and within- group Variance?

> - how to find optimal values of a(linear combinations of variables)

> - 高維度計算效能？

> - Outlier影響嚴不嚴重

# Reference 

- [What is the correct formula for between-class scatter matrix in LDA?](http://stats.stackexchange.com/questions/123490/what-is-the-correct-formula-for-between-class-scatter-matrix-in-lda)

# R labs

<img src="" alt="">
